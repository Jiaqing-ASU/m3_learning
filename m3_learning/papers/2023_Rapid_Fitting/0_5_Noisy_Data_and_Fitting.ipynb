{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5 Generate Noisy Data and Fitting\n",
    "\n",
    "This notebook is used to generate and fit all the data required for the paper. We will generate data for the following noise cases: \n",
    "\n",
    "1, 2, 3, 4, 5, 6, 7 STD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"/home/ferroelectric/Documents/m3_learning/m3_learning/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 19:25:36.041448: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-24 19:25:36.041485: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-24 19:25:36.042167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-24 19:25:36.046853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-24 19:25:36.642250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from m3_learning.be.dataset import BE_Dataset\n",
    "from m3_learning.viz.printing import printer\n",
    "from m3_learning.be.nn import SHO_fit_func_nn, SHO_Model\n",
    "from m3_learning.util.file_IO import download_and_unzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29236116",
   "metadata": {
    "id": "Ol1F9fy7Mo1v"
   },
   "source": [
    "## Loading data for SHO fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f238cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "Or1SY2WdMrkA",
    "outputId": "796c070e-be63-43b5-96f0-e4c30b476c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files already downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download the data file from Zenodo\n",
    "url = 'https://zenodo.org/record/7774788/files/PZT_2080_raw_data.h5?download=1'\n",
    "\n",
    "# Specify the filename and the path to save the file\n",
    "filename = '/data_raw_unmod.h5'\n",
    "save_path = './Data'\n",
    "\n",
    "# download the file\n",
    "download_and_unzip(filename, url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43b4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "├ Measurement_000\n",
      "  ---------------\n",
      "  ├ Channel_000\n",
      "    -----------\n",
      "    ├ Bin_FFT\n",
      "    ├ Bin_Frequencies\n",
      "    ├ Bin_Indices\n",
      "    ├ Bin_Step\n",
      "    ├ Bin_Wfm_Type\n",
      "    ├ Excitation_Waveform\n",
      "    ├ Noise_Floor\n",
      "    ├ Noisy_Data_1\n",
      "    ├ Noisy_Data_2\n",
      "    ├ Noisy_Data_3\n",
      "    ├ Noisy_Data_4\n",
      "    ├ Noisy_Data_5\n",
      "    ├ Noisy_Data_6\n",
      "    ├ Noisy_Data_7\n",
      "    ├ Noisy_Data_8\n",
      "    ├ Position_Indices\n",
      "    ├ Position_Values\n",
      "    ├ Raw_Data\n",
      "    ├ Spatially_Averaged_Plot_Group_000\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Max_Response\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Min_Response\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spatially_Averaged_Plot_Group_001\n",
      "      ---------------------------------\n",
      "      ├ Bin_Frequencies\n",
      "      ├ Max_Response\n",
      "      ├ Mean_Spectrogram\n",
      "      ├ Min_Response\n",
      "      ├ Spectroscopic_Parameter\n",
      "      ├ Step_Averaged_Response\n",
      "    ├ Spectroscopic_Indices\n",
      "    ├ Spectroscopic_Values\n",
      "    ├ UDVS\n",
      "    ├ UDVS_Indices\n",
      "Datasets and datagroups within the file:\n",
      "------------------------------------\n",
      "/\n",
      "/Measurement_000\n",
      "/Measurement_000/Channel_000\n",
      "/Measurement_000/Channel_000/Bin_FFT\n",
      "/Measurement_000/Channel_000/Bin_Frequencies\n",
      "/Measurement_000/Channel_000/Bin_Indices\n",
      "/Measurement_000/Channel_000/Bin_Step\n",
      "/Measurement_000/Channel_000/Bin_Wfm_Type\n",
      "/Measurement_000/Channel_000/Excitation_Waveform\n",
      "/Measurement_000/Channel_000/Noise_Floor\n",
      "/Measurement_000/Channel_000/Noisy_Data_1\n",
      "/Measurement_000/Channel_000/Noisy_Data_2\n",
      "/Measurement_000/Channel_000/Noisy_Data_3\n",
      "/Measurement_000/Channel_000/Noisy_Data_4\n",
      "/Measurement_000/Channel_000/Noisy_Data_5\n",
      "/Measurement_000/Channel_000/Noisy_Data_6\n",
      "/Measurement_000/Channel_000/Noisy_Data_7\n",
      "/Measurement_000/Channel_000/Noisy_Data_8\n",
      "/Measurement_000/Channel_000/Position_Indices\n",
      "/Measurement_000/Channel_000/Position_Values\n",
      "/Measurement_000/Channel_000/Raw_Data\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Bin_Frequencies\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Max_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Mean_Spectrogram\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Min_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Spectroscopic_Parameter\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Step_Averaged_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Bin_Frequencies\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Max_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Mean_Spectrogram\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Min_Response\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Spectroscopic_Parameter\n",
      "/Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Step_Averaged_Response\n",
      "/Measurement_000/Channel_000/Spectroscopic_Indices\n",
      "/Measurement_000/Channel_000/Spectroscopic_Values\n",
      "/Measurement_000/Channel_000/UDVS\n",
      "/Measurement_000/Channel_000/UDVS_Indices\n",
      "\n",
      "The main dataset:\n",
      "------------------------------------\n",
      "<HDF5 file \"data_raw_unmod.h5\" (mode r+)>\n",
      "\n",
      "The ancillary datasets:\n",
      "------------------------------------\n",
      "<HDF5 dataset \"Position_Indices\": shape (3600, 2), type \"<u4\">\n",
      "<HDF5 dataset \"Position_Values\": shape (3600, 2), type \"<f4\">\n",
      "<HDF5 dataset \"Spectroscopic_Indices\": shape (4, 63360), type \"<u4\">\n",
      "<HDF5 dataset \"Spectroscopic_Values\": shape (4, 63360), type \"<f4\">\n",
      "\n",
      "Metadata or attributes in a datagroup\n",
      "------------------------------------\n",
      "BE_actual_duration_[s] : 0.004\n",
      "BE_amplitude_[V] : 1\n",
      "BE_auto_smoothing : auto smoothing on\n",
      "BE_band_edge_smoothing_[s] : 4832.1\n",
      "BE_band_edge_trim : 0.094742\n",
      "BE_band_width_[Hz] : 200000\n",
      "BE_bins_per_band : 0\n",
      "BE_center_frequency_[Hz] : 1310000\n",
      "BE_desired_duration_[s] : 0.004\n",
      "BE_phase_content : chirp-sinc hybrid\n",
      "BE_phase_variation : 1\n",
      "BE_points_per_BE_wave : 0\n",
      "BE_repeats : 4\n",
      "FORC_V_high1_[V] : 1\n",
      "FORC_V_high2_[V] : 10\n",
      "FORC_V_low1_[V] : -1\n",
      "FORC_V_low2_[V] : -10\n",
      "FORC_num_of_FORC_cycles : 1\n",
      "FORC_num_of_FORC_repeats : 1\n",
      "File_MDAQ_version : MDAQ_VS_090915_01\n",
      "File_date_and_time : 18-Sep-2015 18:32:14\n",
      "File_file_name : SP128_NSO\n",
      "File_file_path : C:\\Users\\Asylum User\\Documents\\Users\\Agar\\SP128_NSO\\\n",
      "File_file_suffix : 99\n",
      "IO_AO_amplifier : 10\n",
      "IO_AO_range_[V] : +/- 10\n",
      "IO_Analog_Input_1 : +/- .1V, FFT\n",
      "IO_Analog_Input_2 : off\n",
      "IO_Analog_Input_3 : off\n",
      "IO_Analog_Input_4 : off\n",
      "IO_DAQ_platform : NI 6115\n",
      "IO_rate_[Hz] : 4000000\n",
      "VS_amplitude_[V] : 16\n",
      "VS_cycle_fraction : full\n",
      "VS_cycle_phase_shift : 0\n",
      "VS_measure_in_field_loops : in and out-of-field\n",
      "VS_mode : DC modulation mode\n",
      "VS_number_of_cycles : 2\n",
      "VS_offset_[V] : 0\n",
      "VS_read_voltage_[V] : 0\n",
      "VS_set_pulse_amplitude[V] : 0\n",
      "VS_set_pulse_duration[s] : 0.002\n",
      "VS_step_edge_smoothing_[s] : 0.001\n",
      "VS_steps_per_full_cycle : 96\n",
      "data_type : BEPSData\n",
      "grid_/single : grid\n",
      "grid_contact_set_point_[V] : 1\n",
      "grid_current_col : 1\n",
      "grid_current_row : 1\n",
      "grid_cycle_time_[s] : 10\n",
      "grid_measuring : 0\n",
      "grid_moving : 0\n",
      "grid_num_cols : 60\n",
      "grid_num_rows : 60\n",
      "grid_settle_time_[s] : 0.15\n",
      "grid_time_remaining_[h;m;s] : 10\n",
      "grid_total_time_[h;m;s] : 10\n",
      "grid_transit_set_point_[V] : 0.1\n",
      "grid_transit_time_[s] : 0.15\n",
      "num_bins : 165\n",
      "num_pix : 3600\n",
      "num_udvs_steps : 384\n"
     ]
    }
   ],
   "source": [
    "data_path = save_path + '/' + filename\n",
    "\n",
    "# instantiate the dataset object\n",
    "dataset = BE_Dataset(data_path)\n",
    "\n",
    "# print the contents of the file\n",
    "dataset.print_be_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generates Noisy Data\n",
    "\n",
    "This function will generate noisy records and save them as an h5_main file in the USID format. This allows the data to be computed with the Pycroscopy SHO Fitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0038833667\n"
     ]
    }
   ],
   "source": [
    "# calculates the standard deviation and uses that for the noise\n",
    "noise_STD = np.std(dataset.get_original_data)\n",
    "\n",
    "# prints the standard deviation\n",
    "print(noise_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The STD of the data is: 0.0038833667058497667\n",
      "Adding noise level 1\n",
      "Adding noise level 2\n",
      "Adding noise level 3\n",
      "Adding noise level 4\n",
      "Adding noise level 5\n",
      "Adding noise level 6\n",
      "Adding noise level 7\n",
      "Adding noise level 8\n"
     ]
    }
   ],
   "source": [
    "dataset.generate_noisy_data_records(noise_levels = np.arange(1,9), \n",
    "                                    verbose=True, \n",
    "                                    noise_STD=noise_STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHO fits on all the datasets\n",
    "\n",
    "This will take some time, Each fit takes about 10 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Noisy_Data_1\n",
      "Working on:\n",
      "./Data//data_raw_unmod.h5\n",
      "['Y', 'X'] [60, 60]\n",
      "\n",
      "\n",
      "SHO Fits will be written to:\n",
      "./Data/data_raw_unmod.h5\n",
      "\n",
      "\n",
      "Consider calling test() to check results before calling compute() which computes on the entire dataset and writes results to the HDF5 file\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 7% complete. Time remaining: 2.59 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 15% complete. Time remaining: 2.32 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 23% complete. Time remaining: 2.09 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 31% complete. Time remaining: 1.86 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 39% complete. Time remaining: 1.65 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 47% complete. Time remaining: 1.42 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 54% complete. Time remaining: 1.21 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 62% complete. Time remaining: 59.9 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 70% complete. Time remaining: 47.4 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 78% complete. Time remaining: 34.83 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 86% complete. Time remaining: 22.22 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 94% complete. Time remaining: 9.62 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
      "Finished processing the entire dataset!\n",
      "\n",
      "Note: SHO_Fit has already been performed with the same parameters before. These results will be returned by compute() by default. Set override to True to force fresh computation\n",
      "\n",
      "[<HDF5 group \"/Noisy_Data_1_SHO_Fit/Noisy_Data_1-SHO_Fit_000\" (4 members)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferroelectric/micromamba/envs/paper/lib/python3.10/site-packages/BGlib/be/analysis/fitter.py:140: UserWarning: status dataset not created yet\n",
      "  warn('status dataset not created yet')\n",
      "/home/ferroelectric/micromamba/envs/paper/lib/python3.10/site-packages/BGlib/be/analysis/fitter.py:140: UserWarning: status dataset not created yet\n",
      "  warn('status dataset not created yet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming computation. 0% completed already\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 - 6% complete. Time remaining: 4.98 mins\n",
      "Rank 0 - 13% complete. Time remaining: 3.31 mins\n",
      "Rank 0 - 20% complete. Time remaining: 2.67 mins\n",
      "Rank 0 - 26% complete. Time remaining: 2.27 mins\n",
      "Rank 0 - 33% complete. Time remaining: 1.99 mins\n",
      "Rank 0 - 40% complete. Time remaining: 1.4 mins\n",
      "Rank 0 - 46% complete. Time remaining: 1.25 mins\n",
      "Rank 0 - 53% complete. Time remaining: 1.09 mins\n",
      "Rank 0 - 60% complete. Time remaining: 58.36 sec\n",
      "Rank 0 - 66% complete. Time remaining: 48.23 sec\n",
      "Rank 0 - 73% complete. Time remaining: 39.18 sec\n",
      "Rank 0 - 80% complete. Time remaining: 29.11 sec\n",
      "Rank 0 - 87% complete. Time remaining: 19.34 sec\n",
      "Rank 0 - 93% complete. Time remaining: 8.93 sec\n",
      "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
      "Finished processing the entire dataset!\n",
      "LSQF method took 333.16672372817993 seconds to compute parameters\n",
      "Fitting Noisy_Data_2\n",
      "Working on:\n",
      "./Data//data_raw_unmod.h5\n",
      "['Y', 'X'] [60, 60]\n",
      "\n",
      "\n",
      "SHO Fits will be written to:\n",
      "./Data/data_raw_unmod.h5\n",
      "\n",
      "\n",
      "Consider calling test() to check results before calling compute() which computes on the entire dataset and writes results to the HDF5 file\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 7% complete. Time remaining: 2.49 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 15% complete. Time remaining: 2.28 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 23% complete. Time remaining: 2.07 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 31% complete. Time remaining: 1.86 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 39% complete. Time remaining: 1.65 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 47% complete. Time remaining: 1.43 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 54% complete. Time remaining: 1.22 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 62% complete. Time remaining: 1.01 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 70% complete. Time remaining: 47.74 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 78% complete. Time remaining: 34.98 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 86% complete. Time remaining: 22.35 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 94% complete. Time remaining: 9.69 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
      "Finished processing the entire dataset!\n",
      "\n",
      "Note: SHO_Fit has already been performed with the same parameters before. These results will be returned by compute() by default. Set override to True to force fresh computation\n",
      "\n",
      "[<HDF5 group \"/Noisy_Data_2_SHO_Fit/Noisy_Data_2-SHO_Fit_000\" (4 members)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferroelectric/micromamba/envs/paper/lib/python3.10/site-packages/BGlib/be/analysis/fitter.py:140: UserWarning: status dataset not created yet\n",
      "  warn('status dataset not created yet')\n",
      "/home/ferroelectric/micromamba/envs/paper/lib/python3.10/site-packages/BGlib/be/analysis/fitter.py:140: UserWarning: status dataset not created yet\n",
      "  warn('status dataset not created yet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming computation. 0% completed already\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 - 6% complete. Time remaining: 2.25 mins\n",
      "Rank 0 - 13% complete. Time remaining: 2.04 mins\n",
      "Rank 0 - 20% complete. Time remaining: 1.9 mins\n",
      "Rank 0 - 26% complete. Time remaining: 1.75 mins\n",
      "Rank 0 - 33% complete. Time remaining: 1.61 mins\n",
      "Rank 0 - 40% complete. Time remaining: 1.43 mins\n",
      "Rank 0 - 46% complete. Time remaining: 1.28 mins\n",
      "Rank 0 - 53% complete. Time remaining: 1.11 mins\n",
      "Rank 0 - 60% complete. Time remaining: 56.42 sec\n",
      "Rank 0 - 66% complete. Time remaining: 45.95 sec\n",
      "Rank 0 - 73% complete. Time remaining: 36.96 sec\n",
      "Rank 0 - 80% complete. Time remaining: 27.47 sec\n",
      "Rank 0 - 87% complete. Time remaining: 18.13 sec\n",
      "Rank 0 - 93% complete. Time remaining: 8.87 sec\n",
      "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
      "Finished processing the entire dataset!\n",
      "LSQF method took 320.721284866333 seconds to compute parameters\n",
      "Fitting Noisy_Data_3\n",
      "Working on:\n",
      "./Data//data_raw_unmod.h5\n",
      "['Y', 'X'] [60, 60]\n",
      "\n",
      "\n",
      "SHO Fits will be written to:\n",
      "./Data/data_raw_unmod.h5\n",
      "\n",
      "\n",
      "Consider calling test() to check results before calling compute() which computes on the entire dataset and writes results to the HDF5 file\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 7% complete. Time remaining: 2.49 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 15% complete. Time remaining: 2.27 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 23% complete. Time remaining: 2.06 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 31% complete. Time remaining: 1.85 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 39% complete. Time remaining: 1.64 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 47% complete. Time remaining: 1.43 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 54% complete. Time remaining: 1.22 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 62% complete. Time remaining: 1.01 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 70% complete. Time remaining: 47.9 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 78% complete. Time remaining: 35.14 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 86% complete. Time remaining: 22.41 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 94% complete. Time remaining: 9.72 sec\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
      "Finished processing the entire dataset!\n",
      "\n",
      "Note: SHO_Fit has already been performed with the same parameters before. These results will be returned by compute() by default. Set override to True to force fresh computation\n",
      "\n",
      "[<HDF5 group \"/Noisy_Data_3_SHO_Fit/Noisy_Data_3-SHO_Fit_000\" (4 members)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferroelectric/micromamba/envs/paper/lib/python3.10/site-packages/BGlib/be/analysis/fitter.py:140: UserWarning: status dataset not created yet\n",
      "  warn('status dataset not created yet')\n",
      "/home/ferroelectric/micromamba/envs/paper/lib/python3.10/site-packages/BGlib/be/analysis/fitter.py:140: UserWarning: status dataset not created yet\n",
      "  warn('status dataset not created yet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming computation. 0% completed already\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 - 6% complete. Time remaining: 2.2 mins\n",
      "Rank 0 - 13% complete. Time remaining: 2.02 mins\n",
      "Rank 0 - 20% complete. Time remaining: 1.99 mins\n",
      "Rank 0 - 26% complete. Time remaining: 1.77 mins\n",
      "Rank 0 - 33% complete. Time remaining: 1.61 mins\n",
      "Rank 0 - 40% complete. Time remaining: 1.46 mins\n",
      "Rank 0 - 46% complete. Time remaining: 1.3 mins\n",
      "Rank 0 - 53% complete. Time remaining: 1.1 mins\n",
      "Rank 0 - 60% complete. Time remaining: 57.01 sec\n",
      "Rank 0 - 66% complete. Time remaining: 46.61 sec\n",
      "Rank 0 - 73% complete. Time remaining: 37.27 sec\n",
      "Rank 0 - 80% complete. Time remaining: 27.27 sec\n",
      "Rank 0 - 87% complete. Time remaining: 17.87 sec\n",
      "Rank 0 - 93% complete. Time remaining: 8.6 sec\n",
      "Rank 0 - 100% complete. Time remaining: 0.0 msec\n",
      "Finished processing the entire dataset!\n",
      "LSQF method took 320.8403961658478 seconds to compute parameters\n",
      "Fitting Noisy_Data_4\n",
      "Working on:\n",
      "./Data//data_raw_unmod.h5\n",
      "['Y', 'X'] [60, 60]\n",
      "\n",
      "\n",
      "SHO Fits will be written to:\n",
      "./Data/data_raw_unmod.h5\n",
      "\n",
      "\n",
      "Consider calling test() to check results before calling compute() which computes on the entire dataset and writes results to the HDF5 file\n",
      "\tThis class (likely) supports interruption and resuming of computations!\n",
      "\tIf you are operating in a python console, press Ctrl+C or Cmd+C to abort\n",
      "\tIf you are in a Jupyter notebook, click on \"Kernel\">>\"Interrupt\"\n",
      "\tIf you are operating on a cluster and your job gets killed, re-run the job to resume\n",
      "\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 7% complete. Time remaining: 2.51 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 15% complete. Time remaining: 2.3 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 23% complete. Time remaining: 2.08 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 31% complete. Time remaining: 1.86 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 39% complete. Time remaining: 1.65 mins\n",
      "Rank 0 finished parallel computation\n",
      "Rank 0 - 47% complete. Time remaining: 1.44 mins\n"
     ]
    }
   ],
   "source": [
    "out = [f\"Noisy_Data_{i}\" for i in np.arange(1,9)]\n",
    "out.append(\"Raw_Data\")\n",
    "\n",
    "for data in out:\n",
    "    print(f\"Fitting {data}\")\n",
    "    dataset.SHO_Fitter(dataset = data, h5_sho_targ_grp = f\"{data}_SHO_Fit\", max_mem=1024*64, max_cores= 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks the results to make sure it was saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the contents of the file\n",
    "dataset.print_be_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHOfit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
