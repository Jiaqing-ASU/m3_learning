{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KB7KSC5r1Qar",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial: Cycle-Consistent Spatial Transforming Autoencoders\n",
    "\n",
    "By Shuyu Qin, Joshua C. Agar\n",
    "\n",
    "Department of Mechanical Engineering and Mechanics\n",
    "Drexel University\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIPhuxCqx049",
    "outputId": "a8b871fe-2b6e-4992-d1cb-81ec0300db77",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: m3_learning in c:\\conda\\envs\\tester\\lib\\site-packages (0.0.17)\n",
      "Requirement already satisfied: numpy in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from -r ../tutorial_requirements.txt (line 3)) (3.6.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from -r ../tutorial_requirements.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: torch in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from -r ../tutorial_requirements.txt (line 8)) (9.3.0)\n",
      "Requirement already satisfied: tifffile in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 9)) (2023.8.30)\n",
      "Requirement already satisfied: tqdm in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 11)) (4.66.1)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 13)) (9.0.0)\n",
      "Requirement already satisfied: opencv-python in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 15)) (4.8.0.76)\n",
      "Requirement already satisfied: hyperspy in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 17)) (1.7.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from -r ../tutorial_requirements.txt (line 19)) (8.0.2)\n",
      "Requirement already satisfied: msgpack in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 21)) (1.0.5)\n",
      "Requirement already satisfied: zict in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 23)) (3.0.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 25)) (2.4.0)\n",
      "Requirement already satisfied: pyNSID in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 27)) (0.0.7.2)\n",
      "Requirement already satisfied: pycroscopy in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 29)) (0.63.2)\n",
      "Requirement already satisfied: sidpy in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 31)) (0.12.1)\n",
      "Requirement already satisfied: torchsummary in c:\\conda\\envs\\tester\\lib\\site-packages (from -r ../tutorial_requirements.txt (line 33)) (1.5.1)\n",
      "Collecting wget (from -r ../tutorial_requirements.txt (line 35))\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\conda\\envs\\tester\\lib\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\conda\\envs\\tester\\lib\\site-packages (from matplotlib->-r ../tutorial_requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from sklearn->-r ../tutorial_requirements.txt (line 5)) (1.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r ../tutorial_requirements.txt (line 7)) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\conda\\envs\\tester\\lib\\site-packages (from torch->-r ../tutorial_requirements.txt (line 7)) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\conda\\envs\\tester\\lib\\site-packages (from torch->-r ../tutorial_requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r ../tutorial_requirements.txt (line 7)) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from torch->-r ../tutorial_requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->-r ../tutorial_requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.4.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (1.9.3)\n",
      "Requirement already satisfied: traits>=4.5.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (6.4.2)\n",
      "Requirement already satisfied: natsort in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (8.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.28.2)\n",
      "Requirement already satisfied: dill in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.3.7)\n",
      "Requirement already satisfied: h5py>=2.3 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (3.7.0)\n",
      "Requirement already satisfied: ipyparallel in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (8.6.1)\n",
      "Requirement already satisfied: ipython!=8.0.* in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (8.6.0)\n",
      "Requirement already satisfied: dask[array]>=2.11.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (2022.10.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (2022.10.0)\n",
      "Requirement already satisfied: scikit-image>=0.15 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.21.0)\n",
      "Requirement already satisfied: pint>=0.10 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.22)\n",
      "Requirement already satisfied: numexpr in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.8.5)\n",
      "Requirement already satisfied: sparse in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.14.0)\n",
      "Requirement already satisfied: imageio<2.28 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.27.0)\n",
      "Requirement already satisfied: pyyaml in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (6.0.1)\n",
      "Requirement already satisfied: prettytable in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (3.8.0)\n",
      "Requirement already satisfied: numba>=0.52 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.57.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (6.8.0)\n",
      "Requirement already satisfied: toolz in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.12.0)\n",
      "Requirement already satisfied: zarr>=2.9.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.16.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets->-r ../tutorial_requirements.txt (line 19)) (6.17.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\conda\\envs\\tester\\lib\\site-packages (from ipywidgets->-r ../tutorial_requirements.txt (line 19)) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from ipywidgets->-r ../tutorial_requirements.txt (line 19)) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets->-r ../tutorial_requirements.txt (line 19)) (3.0.3)\n",
      "Requirement already satisfied: cytoolz in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from pyNSID->-r ../tutorial_requirements.txt (line 27)) (0.12.0)\n",
      "Requirement already satisfied: six in c:\\conda\\envs\\tester\\lib\\site-packages (from pyNSID->-r ../tutorial_requirements.txt (line 27)) (1.16.0)\n",
      "Requirement already satisfied: ase in c:\\conda\\envs\\tester\\lib\\site-packages (from pyNSID->-r ../tutorial_requirements.txt (line 27)) (3.22.1)\n",
      "Requirement already satisfied: tensorly>=0.6.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from pycroscopy->-r ../tutorial_requirements.txt (line 29)) (0.8.1)\n",
      "Requirement already satisfied: simpleitk in c:\\conda\\envs\\tester\\lib\\site-packages (from pycroscopy->-r ../tutorial_requirements.txt (line 29)) (2.2.1)\n",
      "Requirement already satisfied: pysptools in c:\\conda\\envs\\tester\\lib\\site-packages (from pycroscopy->-r ../tutorial_requirements.txt (line 29)) (0.15.0)\n",
      "Requirement already satisfied: cvxopt>=1.2.7 in c:\\conda\\envs\\tester\\lib\\site-packages (from pycroscopy->-r ../tutorial_requirements.txt (line 29)) (1.3.2)\n",
      "Requirement already satisfied: distributed>=2.0.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from sidpy->-r ../tutorial_requirements.txt (line 31)) (2022.10.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from sidpy->-r ../tutorial_requirements.txt (line 31)) (5.9.3)\n",
      "Requirement already satisfied: joblib>=0.11.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from sidpy->-r ../tutorial_requirements.txt (line 31)) (1.3.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from dask[array]>=2.11.0->hyperspy->-r ../tutorial_requirements.txt (line 17)) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from dask[array]>=2.11.0->hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.2.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\conda\\envs\\tester\\lib\\site-packages (from dask[array]>=2.11.0->hyperspy->-r ../tutorial_requirements.txt (line 17)) (1.4.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from distributed>=2.0.0->sidpy->-r ../tutorial_requirements.txt (line 31)) (1.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from distributed>=2.0.0->sidpy->-r ../tutorial_requirements.txt (line 31)) (2.0.0)\n",
      "Requirement already satisfied: tornado<6.2,>=6.0.3 in c:\\conda\\envs\\tester\\lib\\site-packages (from distributed>=2.0.0->sidpy->-r ../tutorial_requirements.txt (line 31)) (6.1)\n",
      "Requirement already satisfied: urllib3 in c:\\conda\\envs\\tester\\lib\\site-packages (from distributed>=2.0.0->sidpy->-r ../tutorial_requirements.txt (line 31)) (1.26.16)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\conda\\envs\\tester\\lib\\site-packages (from importlib-metadata>=3.6->hyperspy->-r ../tutorial_requirements.txt (line 17)) (3.16.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (1.6.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (7.3.4)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\conda\\envs\\tester\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (24.0.1)\n",
      "Requirement already satisfied: backcall in c:\\conda\\envs\\tester\\lib\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\conda\\envs\\tester\\lib\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\conda\\envs\\tester\\lib\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.19.0)\n",
      "Requirement already satisfied: pickleshare in c:\\conda\\envs\\tester\\lib\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (3.0.31)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.13.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.6.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\conda\\envs\\tester\\lib\\site-packages (from numba>=0.52->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.40.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image>=0.15->hyperspy->-r ../tutorial_requirements.txt (line 17)) (1.4.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\conda\\envs\\tester\\lib\\site-packages (from scikit-image>=0.15->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from scikit-learn->sklearn->-r ../tutorial_requirements.txt (line 5)) (3.2.0)\n",
      "Requirement already satisfied: asciitree in c:\\conda\\envs\\tester\\lib\\site-packages (from zarr>=2.9.0->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.3.3)\n",
      "Requirement already satisfied: fasteners in c:\\conda\\envs\\tester\\lib\\site-packages (from zarr>=2.9.0->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.18)\n",
      "Requirement already satisfied: numcodecs>=0.10.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from zarr>=2.9.0->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.11.0)\n",
      "Requirement already satisfied: entrypoints in c:\\conda\\envs\\tester\\lib\\site-packages (from ipyparallel->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch->-r ../tutorial_requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: wcwidth in c:\\conda\\envs\\tester\\lib\\site-packages (from prettytable->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from requests->hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from requests->hyperspy->-r ../tutorial_requirements.txt (line 17)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\conda\\envs\\tester\\lib\\site-packages (from requests->hyperspy->-r ../tutorial_requirements.txt (line 17)) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\conda\\envs\\tester\\lib\\site-packages (from sympy->torch->-r ../tutorial_requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\conda\\envs\\tester\\lib\\site-packages (from jedi>=0.16->ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (4.11.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from stack-data->ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\joshua agar\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in c:\\conda\\envs\\tester\\lib\\site-packages (from stack-data->ipython!=8.0.*->hyperspy->-r ../tutorial_requirements.txt (line 17)) (0.2.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\conda\\envs\\tester\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->-r ../tutorial_requirements.txt (line 19)) (305.1)\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "zenodopy 0.3.0 requires types-requests>=2, which is not installed.\n",
      "m3-learning 0.0.17 requires imutils, which is not installed.\n",
      "m3-learning 0.0.17 requires jupyter-book, which is not installed.\n",
      "m3-learning 0.0.17 requires plotly, which is not installed.\n",
      "m3-learning 0.0.17 requires xrayutilities, which is not installed.\n",
      "m3-learning 0.0.17 requires hyperspy==1.7.4, but you have hyperspy 1.7.5 which is incompatible.\n",
      "m3-learning 0.0.17 requires imageio==2.22.3, but you have imageio 2.27.0 which is incompatible.\n",
      "m3-learning 0.0.17 requires numpy==1.23.4, but you have numpy 1.24.4 which is incompatible.\n",
      "m3-learning 0.0.17 requires scikit-image==0.19.3, but you have scikit-image 0.21.0 which is incompatible.\n",
      "m3-learning 0.0.17 requires sidpy==0.11, but you have sidpy 0.12.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install m3_learning --no-deps\n",
    "!pip install -r ../tutorial_requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GbJmyO_1itx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Configuration\n",
    "\n",
    "### Imports packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "29lMNJV9xe7w",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from skimage.filters import sobel\n",
    "import tarfile\n",
    "import wget\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8R1uHT1P1sPq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Downloads Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cards.tar.xz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://github.com/m3-learning/m3_learning/blob/main/m3_learning/Tutorials/Cycle_Consistent_Spatial_Transformer_Autoencoder/data/cards.tar.xz?raw=true', 'cards.tar.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8V3z9ZZFyUiu",
    "outputId": "28a3773f-3eb6-40da-b859-30056d8e1767",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "!wget - O cards.tar.xz \n",
    "!wget - O 11.12_unsupervised_learn_label_epoch_17957_coef_0_trainloss_0.0008.pkl https: // github.com/m3-learning/Unsupervised-rotation-detection-and -label-learning/blob/main/11.12_unsupervised_learn_label_epoch_17957_coef_0_trainloss_0.0008.pkl?raw = true\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XGnsP5VK14Wa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Extracts Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFQrlSJLyiMm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "file = tarfile.open('./cards.tar.xz')\n",
    "file.extractall('./')\n",
    "file.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-3AkKueg2IWu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4At_FTxxe7x",
    "outputId": "011c308f-7b07-45b4-995d-7136faee87ee",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Converts images to grayscale\n",
    "card1 = cv2.imread(\"cards/card1.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "card2 = cv2.imread(\"cards/card2.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "card3 = cv2.imread(\"cards/card3.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "card4 = cv2.imread(\"cards/card4.JPG\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Normalizes the image based on the max value\n",
    "card1 = torch.tensor(1 - card1 / card1.max())\n",
    "card2 = torch.tensor(1 - card2 / card2.max())\n",
    "card3 = torch.tensor(1 - card3 / card3.max())\n",
    "card4 = torch.tensor(1 - card4 / card4.max())\n",
    "\n",
    "# Resizes the image\n",
    "card_small_1 = F.interpolate(card1.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n",
    "card_small_2 = F.interpolate(card2.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n",
    "card_small_3 = F.interpolate(card3.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n",
    "card_small_4 = F.interpolate(card4.unsqueeze(0).unsqueeze(\n",
    "    1), size=(48, 48)).clone().type(torch.float).cpu().numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "diJ86fln2W69",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Raw Data Visulalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "MzZTavM5xe7y",
    "outputId": "62889007-1bcc-4f98-80d4-76e1cf1fb7ef",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(card_small_1.squeeze())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tQU-TTGs2jWy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edge detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBgdUW0Exe7y",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Sobel edge detection\n",
    "card_edge_1 = sobel(card_small_1.squeeze())\n",
    "card_edge_2 = sobel(card_small_2.squeeze())\n",
    "card_edge_3 = sobel(card_small_3.squeeze())\n",
    "card_edge_4 = sobel(card_small_4.squeeze())\n",
    "\n",
    "# Convert to a tensor\n",
    "card_edge_1 = torch.tensor(\n",
    "    card_edge_1, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n",
    "card_edge_2 = torch.tensor(\n",
    "    card_edge_2, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n",
    "card_edge_3 = torch.tensor(\n",
    "    card_edge_3, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n",
    "card_edge_4 = torch.tensor(\n",
    "    card_edge_4, dtype=torch.float).unsqueeze(0).unsqueeze(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mubTbD6Y2xEH",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "This will generate a bunch of images that are rotated from the initial position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYROdDpxxe7y",
    "outputId": "e0df3053-3ec5-41b4-bb9f-3776825fc481",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# sets the range and the number of steps to generate images\n",
    "angle = torch.linspace(0, 2*np.pi, 2000)\n",
    "\n",
    "# predefines a list\n",
    "input_data_set_1 = []\n",
    "input_data_set_2 = []\n",
    "input_data_set_3 = []\n",
    "input_data_set_4 = []\n",
    "\n",
    "for q in angle:\n",
    "    # defines the theta matrix for an affine transformation\n",
    "    theta = torch.tensor([\n",
    "        [torch.cos(q), torch.sin(q), 0],\n",
    "        [-torch.sin(q), torch.cos(q), 0]\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "    # calculates the grid for the affine transformation\n",
    "    grid = F.affine_grid(theta.unsqueeze(0), card_edge_1.size())\n",
    "\n",
    "    # applies the affine transformation to the image\n",
    "    output = F.grid_sample(card_edge_1, grid)\n",
    "    # adds the image to the dataset\n",
    "    input_data_set_1.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_2, grid)\n",
    "    input_data_set_2.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_3, grid)\n",
    "    input_data_set_3.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_4, grid)\n",
    "    input_data_set_4.append(output)\n",
    "\n",
    "# combines all the data together\n",
    "input_set_1 = torch.stack(input_data_set_1).squeeze(1)\n",
    "input_set_2 = torch.stack(input_data_set_2).squeeze(1)\n",
    "input_set_3 = torch.stack(input_data_set_3).squeeze(1)\n",
    "input_set_4 = torch.stack(input_data_set_4).squeeze(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ob48pgqA3cWN",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize example rotated images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "Sk_uczFjxe7y",
    "outputId": "bfa01918-0748-40ad-bedc-7a125f05371d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(card_edge_1.squeeze())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RkBjGr7B3lTY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Formulate a single dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDZZp451xe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_set = torch.cat(\n",
    "    (input_set_1, input_set_2, input_set_3, input_set_4), axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jDjcsxgV3vIT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualize example images in training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ocRbDC50xe7z",
    "outputId": "2b5aa34c-7ad8-43be-bd65-f3663605da48",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    if i % 200 == 0:\n",
    "        plt.figure()\n",
    "        plt.imshow(input_set_4[i].squeeze())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TedTGO8S35AE",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Builds the Neural Network\n",
    "\n",
    "### Convolutional Block\n",
    "\n",
    "This is a standard convolutional block with ReLu. Each block has 4 layers. There is a layer normalization, and ResNet-like message passing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXCJoAUexe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, t_size, n_step):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.cov1d_1 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.cov1d_2 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.cov1d_3 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.norm_3 = nn.LayerNorm(n_step)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.relu_3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        out = self.cov1d_1(x)\n",
    "        out = self.relu_1(out)\n",
    "        out = self.cov1d_2(out)\n",
    "        out = self.relu_2(out)\n",
    "        out = self.cov1d_3(out)\n",
    "        out = self.norm_3(out)\n",
    "        out = self.relu_3(out)\n",
    "        out = out.add(x_input)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R0SqTPPh4Tiy",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Idenity Block\n",
    "\n",
    "This is a single convolutional layer with a layer norm and ReLu activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U86eWxQNxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class identity_block(nn.Module):\n",
    "    def __init__(self, t_size, n_step):\n",
    "        super(identity_block, self).__init__()\n",
    "        self.cov1d_1 = nn.Conv2d(\n",
    "            t_size, t_size, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "        self.norm_1 = nn.LayerNorm(n_step)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        out = self.cov1d_1(x)\n",
    "        out = self.norm_1(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QiudNnwF4n2j",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder construction\n",
    "\n",
    "This constructs the encoder using the convolutional and identity blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6MSmcRCxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, original_step_size, pool_list, embedding_size, conv_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # list of blocks\n",
    "        blocks = []\n",
    "\n",
    "        # defines the orignal step size\n",
    "        self.input_size_0 = original_step_size[0]\n",
    "        self.input_size_1 = original_step_size[1]\n",
    "\n",
    "        # a list that has the number of pooling layers\n",
    "        number_of_blocks = len(pool_list)\n",
    "\n",
    "        # Adds an initial Conv_block, identity block and max pool\n",
    "        blocks.append(conv_block(t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(identity_block(\n",
    "            t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(nn.MaxPool2d(pool_list[0], stride=pool_list[0]))\n",
    "\n",
    "        # adds additional layers based on the number of blocks\n",
    "        for i in range(1, number_of_blocks):\n",
    "            original_step_size = [\n",
    "                original_step_size[0]//pool_list[i-1], original_step_size[1]//pool_list[i-1]]\n",
    "            blocks.append(conv_block(t_size=conv_size,\n",
    "                          n_step=original_step_size))\n",
    "            blocks.append(identity_block(\n",
    "                t_size=conv_size, n_step=original_step_size))\n",
    "            blocks.append(nn.MaxPool2d(pool_list[i], stride=pool_list[i]))\n",
    "\n",
    "        # defines the convolutional embedding blocks\n",
    "        self.block_layer = nn.ModuleList(blocks)\n",
    "\n",
    "        self.layers = len(blocks)\n",
    "\n",
    "        original_step_size = [original_step_size[0] //\n",
    "                              pool_list[-1], original_step_size[1]//pool_list[-1]]\n",
    "\n",
    "        input_size = original_step_size[0]*original_step_size[1]\n",
    "\n",
    "        # defines the initial layer\n",
    "        self.cov2d = nn.Conv2d(1, conv_size, 3, stride=1,\n",
    "                               padding=1, padding_mode='zeros')\n",
    "\n",
    "        # defines the conv layer at the end of the conv block\n",
    "        self.cov2d_1 = nn.Conv2d(\n",
    "            conv_size, 1, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Layer that takes the ouput of the conv block and reduces its dimensionsl to 20\n",
    "        self.before = nn.Linear(input_size, 20)\n",
    "\n",
    "        # layer that takes the 20 parameter latent space and learns the embedding of the affine transformation\n",
    "        self.dense = nn.Linear(20, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x.view(-1, 1, self.input_size_0, self.input_size_1)\n",
    "        out = self.cov2d(out)\n",
    "        for i in range(self.layers):\n",
    "            out = self.block_layer[i](out)\n",
    "        out = self.cov2d_1(out)\n",
    "\n",
    "        # flattens the conv layers so it is 1D\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "\n",
    "        # Embedding that goes to the classification layer\n",
    "        kout = self.before(out)\n",
    "\n",
    "        # Fully connected layer that helps learn the affine transformation\n",
    "        out = self.dense(kout)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        theta = out.view(-1, 2, 3)\n",
    "\n",
    "        # learns the grid of the affine tranformation\n",
    "        grid = F.affine_grid(theta.to(device), x.size()).to(device)\n",
    "\n",
    "        # applies the affine transformation to the image\n",
    "        output = F.grid_sample(x, grid)\n",
    "\n",
    "        return output, kout, theta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bRPxUQed6cbu",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyQJ9qQsxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, original_step_size, up_list, embedding_size, conv_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Defines the size of the input\n",
    "        self.input_size_0 = original_step_size[0]\n",
    "        self.input_size_1 = original_step_size[1]\n",
    "\n",
    "        # dense layer used for the probability of beloning to each class\n",
    "        self.dense = nn.Linear(4, original_step_size[0]*original_step_size[1])\n",
    "\n",
    "        self.cov2d = nn.Conv2d(1, conv_size, 3, stride=1,\n",
    "                               padding=1, padding_mode='zeros')\n",
    "        self.cov2d_1 = nn.Conv2d(\n",
    "            conv_size, 1, 3, stride=1, padding=1, padding_mode='zeros')\n",
    "\n",
    "        # list where the blocks are saved\n",
    "        blocks = []\n",
    "\n",
    "        # number of blocks in the model\n",
    "        number_of_blocks = len(up_list)\n",
    "\n",
    "        # adds the blocks to the model\n",
    "        blocks.append(conv_block(t_size=conv_size, n_step=original_step_size))\n",
    "        blocks.append(identity_block(\n",
    "            t_size=conv_size, n_step=original_step_size))\n",
    "\n",
    "        for i in range(number_of_blocks):\n",
    "            # adds an upsampling layer to compensate for pooling\n",
    "            blocks.append(nn.Upsample(\n",
    "                scale_factor=up_list[i], mode='bilinear', align_corners=True))\n",
    "            original_step_size = [original_step_size[0] *\n",
    "                                  up_list[i], original_step_size[1]*up_list[i]]\n",
    "            blocks.append(conv_block(t_size=conv_size,\n",
    "                          n_step=original_step_size))\n",
    "            blocks.append(identity_block(\n",
    "                t_size=conv_size, n_step=original_step_size))\n",
    "\n",
    "        self.block_layer = nn.ModuleList(blocks)\n",
    "        self.layers = len(blocks)\n",
    "        self.output_size_0 = original_step_size[0]\n",
    "        self.output_size_1 = original_step_size[1]\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(4)\n",
    "\n",
    "        # used to convert probability into classification\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # prediction layer for classification\n",
    "        self.for_k = nn.Linear(20, 4)\n",
    "\n",
    "        # number of outputs for the classification layer\n",
    "        self.num_k_sparse = 1\n",
    "\n",
    "    def ktop(self, x):\n",
    "        # This conduct the classification based on the encoded value\n",
    "        kout = self.for_k(x)\n",
    "        kout = self.norm(kout)\n",
    "        kout = self.softmax(kout)\n",
    "        k_no = kout.clone()\n",
    "        k = self.num_k_sparse\n",
    "        with torch.no_grad():\n",
    "            if k < kout.shape[1]:\n",
    "                for raw in k_no:\n",
    "                    # computes the k-top layer\n",
    "                    indices = torch.topk(raw, k)[1].to(device)\n",
    "\n",
    "                    # creates a one-hot encoded vector\n",
    "                    mask = torch.ones(raw.shape, dtype=bool).to(device)\n",
    "                    mask[indices] = False\n",
    "                    raw[mask] = 0\n",
    "                    raw[~mask] = 1\n",
    "        return k_no\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Does the classification\n",
    "        k_out = self.ktop(x)\n",
    "\n",
    "        # uses the classification to the decoder\n",
    "        out = self.dense(k_out)\n",
    "\n",
    "        # reshapes the tensor to be an image of the size of the original image\n",
    "        out = out.view(-1, 1, self.input_size_0, self.input_size_1)\n",
    "\n",
    "        # computes the decoder\n",
    "        out = self.cov2d(out)\n",
    "        for i in range(self.layers):\n",
    "            out = self.block_layer[i](out)\n",
    "        out = self.cov2d_1(out)\n",
    "        out = self.relu_1(out)\n",
    "\n",
    "        return out, k_out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H06vwqpPxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Joint(nn.Module):\n",
    "  # Module that combines the encoder and the decoder\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Joint, self).__init__()\n",
    "\n",
    "        # encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # gets the result from the encoder\n",
    "        predicted, kout, theta = self.encoder(x)\n",
    "\n",
    "        # Builds the theta matrix\n",
    "        # We use an identity to ensure there is no translation\n",
    "        identity = torch.tensor([0, 0, 1], dtype=torch.float).reshape(\n",
    "            1, 1, 3).repeat(x.shape[0], 1, 1).to(device)\n",
    "        new_theta = torch.cat((theta, identity), axis=1).to(device)\n",
    "\n",
    "        # Computes the inverse of the affine transoformation\n",
    "        inver_theta = torch.linalg.inv(new_theta)[:, 0:2].to(device)\n",
    "\n",
    "        # Calculates the grid for inverse affine\n",
    "        grid = F.affine_grid(inver_theta.to(device), x.size()).to(device)\n",
    "\n",
    "        # Computes the decoder\n",
    "        predicted_base, k_out = self.decoder(kout)\n",
    "\n",
    "        # applies the inverse affine transformation to the decoded base\n",
    "        predicted_input = F.grid_sample(predicted_base, grid)\n",
    "\n",
    "        return predicted, predicted_base, predicted_input, k_out, theta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sets optional parameters for autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lIFzkPoxe7z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Size of the original image\n",
    "en_original_step_size = [48, 48]\n",
    "\n",
    "# Defines the pooling layer\n",
    "pool_list = [2, 2, 2]\n",
    "\n",
    "# defines the size of the tensor\n",
    "de_original_step_size = [4, 4]\n",
    "\n",
    "# Defines how the upsampling should be conducted\n",
    "up_list = [2, 2, 3]\n",
    "\n",
    "# Sets the size of the embedding, this is the number of parameters in the affine\n",
    "embedding_size = 6\n",
    "\n",
    "# Sets the number of neurons in each layer\n",
    "conv_size = 128\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LpPxYC3d-2xp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Checks operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjixlRluxe70",
    "outputId": "2c5df336-8a9a-4b0a-bdcf-5fe94ffa7b1f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# checks if cuda is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Assigns device to cuda\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj-kWD7dAf86",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instantiates the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBqkK_6qxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# instantiates the encoder\n",
    "encoder = Encoder(original_step_size=en_original_step_size,\n",
    "                  pool_list=pool_list,\n",
    "                  embedding_size=embedding_size,\n",
    "                  conv_size=conv_size).to(device)\n",
    "\n",
    "# instantiates the decoder\n",
    "decoder = Decoder(original_step_size=de_original_step_size,\n",
    "                  up_list=up_list,\n",
    "                  embedding_size=embedding_size,\n",
    "                  conv_size=conv_size).to(device)\n",
    "\n",
    "# combines the two models\n",
    "join = Joint(encoder, decoder).to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RrNEt8A1Av7s",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Instantiate the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGrcC1vFxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(join.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loads pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D-ngUHDxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# if you would like to train this model from scratch set equal to false.\n",
    "# Note this model takes many hours to train\n",
    "pretrained = True\n",
    "\n",
    "if pretrained:\n",
    "    # load the trained weights\n",
    "    path_checkpoint = \"./11.12_unsupervised_learn_label_epoch_17957_coef_0_trainloss_0.0008.pkl\"\n",
    "    checkpoint = torch.load(path_checkpoint, map_location=torch.device('cpu'))\n",
    "\n",
    "    join.load_state_dict(checkpoint['net'])\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LBx-rQ8hBGro",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9-_12ivxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(join,\n",
    "                  train_iterator,\n",
    "                  optimizer,\n",
    "                  ln_parm=1,\n",
    "                  beta=None):\n",
    "\n",
    "    # weight_decay = coef\n",
    "    # weight_decay_1 = coef1\n",
    "\n",
    "    # set the train mode\n",
    "    join.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # loop for calculating loss\n",
    "    for x in tqdm(train_iterator, leave=True, total=len(train_iterator)):\n",
    "\n",
    "        x = x.to(device, dtype=torch.float)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # computes the forward pass of the model\n",
    "        predicted_x, predicted_base, predicted_input, kout, theta = join(x)\n",
    "\n",
    "        # combines the loss from the affine and inverse affine transform\n",
    "        loss = F.mse_loss(predicted_base.squeeze(), predicted_x.squeeze(), reduction='mean')\\\n",
    "            + F.mse_loss(predicted_input.squeeze(),\n",
    "                         x.squeeze(), reduction='mean')\n",
    "\n",
    "        # backward pass\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # computes the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-sgaRX8EBur2",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZzaf6qexe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def Train(join, encoder, decoder, train_iterator, optimizer,\n",
    "          epochs, coef=0, coef_1=0, ln_parm=1, beta=None, epoch_=None):\n",
    "\n",
    "    # sets the number of epochs to train\n",
    "    N_EPOCHS = epochs\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "\n",
    "    if epoch_ == None:\n",
    "        start_epoch = 0\n",
    "    else:\n",
    "        start_epoch = epoch_+1\n",
    "\n",
    "    # loops around the data set for each epoch\n",
    "    for epoch in range(start_epoch, N_EPOCHS):\n",
    "\n",
    "        # computes the loss\n",
    "        train = loss_function(join, train_iterator,\n",
    "                              optimizer, ln_parm, beta)\n",
    "\n",
    "        # updates and prints the loss\n",
    "        train_loss = train\n",
    "        train_loss /= len(train_iterator)\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}')\n",
    "        print('.............................')\n",
    "\n",
    "        # If the model shows improvement after 50 epochs will save\n",
    "        if best_train_loss > train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            patience_counter = 1\n",
    "            checkpoint = {\n",
    "                \"net\": join.state_dict(),\n",
    "                \"encoder\": encoder.state_dict(),\n",
    "                \"decoder\": decoder.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "            if epoch >= 50:\n",
    "                torch.save(\n",
    "                    checkpoint, f'/content/drive/MyDrive/cycle_AE_model/11.12_unsupervised_learn_label_epoch:{epoch}_coef:{coef}_trainloss:{train_loss:.4f}.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Ez-cMgCPWx",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data loader\n",
    "\n",
    "Defines an iterator that serves as the data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leIGbHF5xe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(\n",
    "    input_set, batch_size=300, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "da4PsSkrCalq",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trains the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTc9_MYoxe70",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if not pretrained:\n",
    "    # unsupervised learn label\n",
    "    Train(join, encoder, decoder, train_iterator, optimizer, 50000, epoch_=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nv1qu_-CjMd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "### Generates a small dataset for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOhh6qPhxe73",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# generate the test data set for validation\n",
    "angle = torch.linspace(0, 7*np.pi, 25)\n",
    "\n",
    "input_data_set_1 = []\n",
    "input_data_set_2 = []\n",
    "input_data_set_3 = []\n",
    "input_data_set_4 = []\n",
    "\n",
    "for q in angle:\n",
    "    theta = torch.tensor([\n",
    "        [torch.cos(q), torch.sin(q), 0],\n",
    "        [-torch.sin(q), torch.cos(q), 0]\n",
    "    ], dtype=torch.float)\n",
    "    grid = F.affine_grid(theta.unsqueeze(0), card_edge_1.size())\n",
    "    output = F.grid_sample(card_edge_1, grid)\n",
    "    input_data_set_1.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_2, grid)\n",
    "    input_data_set_2.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_3, grid)\n",
    "    input_data_set_3.append(output)\n",
    "\n",
    "    output = F.grid_sample(card_edge_4, grid)\n",
    "    input_data_set_4.append(output)\n",
    "\n",
    "input_set_1 = torch.stack(input_data_set_1).squeeze(1)\n",
    "input_set_2 = torch.stack(input_data_set_2).squeeze(1)\n",
    "input_set_3 = torch.stack(input_data_set_3).squeeze(1)\n",
    "input_set_4 = torch.stack(input_data_set_4).squeeze(1)\n",
    "input_set_small = torch.cat(\n",
    "    (input_set_1, input_set_2, input_set_3, input_set_4), axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OTLbu0YNDTNA",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds a validation iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5wp3qyGxe73",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(\n",
    "    input_set_small, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FIGtuCR0DWZe",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random sample visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywERve9Vxe73",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample = next(iter(train_iterator))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vfQlibj2De5f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrrJbpk8xe73",
    "outputId": "ed0710f7-29a6-4847-93e4-7c12e1c78e00",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "out, base, inp, kout, theta = join(sample.to(device, dtype=torch.float))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VGkr2MlRDmiV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fvmZkiI8xe73",
    "outputId": "3145b36f-2514-471d-d3bf-44e4ce417d6f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "label_ = [card_edge_1, card_edge_2, card_edge_3, card_edge_4]\n",
    "\n",
    "# Visualize the result\n",
    "fig, ax = plt.subplots(6, 4, figsize=(20, 30))\n",
    "for i in range(6):\n",
    "    j = np.random.randint(0, 100)\n",
    "    ax[i][0].title.set_text('input image')\n",
    "    ax[i][1].title.set_text('affine transform')\n",
    "    ax[i][2].title.set_text('generated basis')\n",
    "    ax[i][3].title.set_text(\n",
    "        f'Rotation {np.degrees(np.arccos(theta[j][0,0].detach().cpu()))}')\n",
    "    ax[i][0].imshow(sample[j].squeeze())\n",
    "\n",
    "    ax[i][1].imshow(out[j].squeeze().detach().cpu())\n",
    "\n",
    "    # ax[2].imshow((card_small.squeeze()-out[i].squeeze().detach().cpu())**2)\n",
    "    # num = torch.argmax(prob[i])\n",
    "    ax[i][2].imshow(base[j].squeeze().detach().cpu())\n",
    "\n",
    "    ax[i][3].imshow(inp[j].squeeze().detach().cpu())\n",
    "\n",
    "    # ax[3].imshow((label_[num].squeeze()-out[i].squeeze().detach().cpu())**2)\n",
    "    print(kout[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmwFiBWamGVb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "28d6d9e31a694f2aba28d42b1019d9365f56410d563150feaee59905aa4508a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
